Q1.  What is an Operating System & Types of OS

    An operating system (OS) is software that acts as an interface between computer hardware and user applications
    It manages the computer's resources, such as the CPU, memory, disk storage, and peripheral devices (like printers), and ensures that different applications and users can interact with the system efficiently and securely.


    ### Key Functions of an Operating System:

    *Process Management: The OS manages the execution of programs. It decides which processes (programs in execution) get to use the CPU, when, and for how long.

    *Memory Management: The OS keeps track of each byte of memory and ensures that each process has enough memory to run. It also handles swapping between main memory and disk when memory is full.

    *File System Management: It provides an organized way to store, retrieve, and manage data on disks (such as hard drives or SSDs).

    *Device Management: The OS allows interaction between hardware devices (like a keyboard, mouse, or printer) and software. It controls drivers that communicate with these devices.

    *Security and Access Control: The OS manages user permissions and security protocols, making sure only authorized users can access the system or sensitive data.

    *User Interface: OS provides an interface, either graphical (GUI) like in Windows, or command-line (CLI) like in Linux.


    ### Example: Windows Operating System
    Imagine you're using a computer with Windows 10:

    *You launch a web browser (say Chrome) to browse the internet.

    *The process management feature of the OS makes sure Chrome gets time on the CPU to run while managing other programs, like a music player or document editor, running in the background.

    *You open several tabs in Chrome. The memory management feature of the OS allocates memory (RAM) to each tab to load websites while ensuring other applications also have enough memory.

    *You download a file from the internet. The file system management ensures that the file is saved in the correct folder (e.g., Downloads), and allows you to organize, rename, or move the file.

    *You print a document. The device management communicates with the printer driver, ensuring the document is printed correctly without requiring you to manually control the printer.

    *Your account has a password. The security feature ensures that only users with the correct credentials can log in and access the data on your system.

    In this way, the OS ensures that all the hardware and software resources of the computer are used efficiently, providing a smooth experience for the user.


    ### There are several types of operating systems, including:

    1. Windows: Developed by Microsoft,
    2. macOS: Developed by Apple
    3. Linux: Linux is an open-source operating system
    4. Android: Developed by Google
    5. iOS: Developed by Apple


Q2.  Process vs Threads vs Programs

    *A program is simply a set of instructions written in a programming language. A file on disk that contains executable code. When you double-click or run a program, it becomes a process.

    Example: We have Google Chrome installed on your computer. It’s just a program (set of instructions stored on your disk) until you open it.

    *A process is an instance of a program in execution. When a program is executed, the operating system creates a process. A process has its own memory space and resources, like CPU time and memory, allocated to it. Multiple processes can be running the same program at once, but each will be independent of one another.

    Example: When you open Google Chrome, the program turns into a process. If you open multiple Chrome windows, each of these is a separate process of the same program. Each process has its own memory space, which means they run independently—if one crashes, the others can still work.

    *A thread is the smallest unit of execution within a process. A single process can have multiple threads running concurrently, all sharing the same memory space and resources. This allows for more efficient CPU usage since threads within the same process can run simultaneously without needing as much overhead as managing multiple processes.

    Example: In Google Chrome, each tab can be a separate thread within the same process. If one tab freezes or crashes, it might not affect the entire browser because Chrome can use multi-threading to manage each tab independently.


Q3.  Difference between Multiprogramming, Multiprocess, Multitasking, and Multithreading

    *Multiprogramming refers to the ability of the operating system to load multiple programs into memory simultaneously and manage them in such a way that while one program is waiting for some event (e.g., user input or disk operation), the CPU can switch to another program and execute it. 

    Example: You are running a media player to play music, and at the same time, you have a word processor (e.g., MS Word) open. The OS can switch between the media player and the word processor depending on which one is waiting for resources. For example, when the word processor is waiting for user input (e.g., typing), the OS can switch the CPU to the media player to keep playing music.

    *Multiprocessing refers to the use of two or more CPUs (or cores) in a single computer system. This allows the system to run multiple processes simultaneously on different processors

    Example: If you have a computer with a quad-core processor, you can run multiple programs (or processes) concurrently. For instance, you can have a web browser, a game, and a video editor running at the same time. Since the system has multiple cores, each core can handle a separate process, so these programs can run truly in parallel rather than being switched between a single CPU core.

    *Multitasking refers to the ability of an operating system to handle multiple tasks (processes) at the same time. There are two types of multitasking.
        Example: You are browsing the web, listening to music, and downloading a file all at the same time. The OS handles switching between these tasks quickly enough that it seems as though they are happening simultaneously. You may be typing in the browser, but the music player is still playing in the background, and the download is continuing.

    *Multithreading refers to the ability of a process to execute multiple threads concurrently. Each thread is a sequence of instructions, and they share the same memory space. Multithreading increases the efficiency of a process by performing multiple operations simultaneously within a single process.

    Example:
    Take Microsoft Word as an example. While you are typing in a document, Word may have several threads running:

    One thread is handling text input.
    Another thread is checking the grammar.
    Another thread is autosaving the document in the background.

Q4. Various Process States

    State	        Description
    New	            Process is being created.
    Ready	        Process is waiting for CPU time.
    Running	        Process is currently being executed by the CPU.
    Blocked	        Process is waiting for an event (e.g., I/O operation) to complete.
    Terminated	    Process has finished execution or is stopped.
    Suspended	    Process is temporarily moved to secondary storage due to memory constraints.

    Example:
    1. New:When you open a new program like Google Chrome, it initially starts in the New state as it’s being initialized by the OS.
    2. Ready: Chrome is loaded into memory and is waiting for the CPU to allocate time to it, but it hasn’t started executing yet.
    3. Running:When Chrome is actively being executed by the CPU (e.g., rendering a webpage), it’s in the Running state.
    4. Blocked (or Waiting):If Chrome is waiting for data from the network (e.g., waiting for a webpage to load), it enters the Blocked state until the data arrives.
    5. Terminated:When you close Chrome or it finishes all its tasks, it enters the Terminated state.
    6.  Suspended (or Swapped Out): If your system is running low on memory, the OS might suspend a background process like a large file download and move it to disk until more memory becomes available.

Q5. CPU scheduling Algorithms

    CPU scheduling algorithms determine how processes are assigned to the CPU in an efficient way to make the most of CPU utilization

    1. First-Come, First-Served (FCFS): The simplest scheduling algorithm. Processes are executed in the order they arrive in the ready queue. The first process that arrives is the first one to be executed.
        Advantages: Simple to implement.
        Disadvantages:Leads to the convoy effect: Shorter processes can get stuck behind longer ones, resulting in poor waiting time for small processes.
        *Non-preemptive

    2. Shortest Job First (SJF):The process with the shortest burst time is selected first for execution.
        Advantages:Minimizes average waiting time.
        Disadvantages:Can lead to starvation of long processes if many short processes keep arriving.
        *Non-preemptive

    3. Round Robin (RR): Each process is assigned a time quantum (time slice). When a process's time slice is up, it is preempted, and the next process is given a turn. The preempted process is placed back in the ready queue to wait for its next turn.
    Type: Preemptive.
    Advantages:Fair scheduling, as each process gets an equal time slice.
    Disadvantages:The performance depends heavily on the length of the time quantum. A poorly chosen time quantum can lead to inefficiencies.

    4. Priority Scheduling:Each process is assigned a priority, and the process with the highest priority is executed first. Processes with the same priority are handled using FCFS.
        Advantages:Good for processes with different levels of importance.
        Disadvantages:Can lead to starvation of lower-priority processes.

    5. Multilevel Queue Scheduling:Processes are divided into multiple queues based on their type and each queue has its own scheduling algorithm. The queues themselves can be scheduled based on priority or time slices.

    Example:
    You can have two queues: one for system processes (higher priority) and one for user processes (lower priority).

    The system queue might use Priority Scheduling.
    The user queue might use Round Robin.

    If a system process arrives, it will preempt any user process, ensuring that system tasks (like handling interrupts) are completed first.

    ### Preemptive and Non-preemptive

    *In preemptive scheduling, the operating system can forcibly remove a running process from the CPU and allocate the CPU to another process.

    *In non-preemptive scheduling, once a process starts its execution on the CPU, it cannot be forcibly removed until it completes its task

Q6. Critical section Problem

    The Critical Section Problem is a fundamental issue in concurrent programming that arises when multiple processes or threads need to access shared resources. The problem is to ensure that when one process is using a shared resource, no other process can access that resource at the same time.

    Real-life Example:
    Imagine a banking system where two users are trying to withdraw money from a shared bank account. If both users access the account balance simultaneously and attempt to withdraw without proper synchronization, they might both think they have enough balance, leading to both withdrawals being processed incorrectly.

    ### Race Condition: A situation Occur when two or more processes access shared resources at the same time, and the outcome depends on the order in which the processes execute.

        Example: Two bank customers attempting to withdraw money from the same account at the same time without synchronization could result in incorrect balance calculations.

Q7.  Process synchronisation

    Process Synchronization is a concept in operating systems that ensures that multiple processes or threads can execute concurrently without interfering with each other, especially when they access shared resources. The goal of process synchronization is to prevent race conditions and ensure data consistency and correct execution in multi-process or multi-threaded environments.

        Data Consistency: Ensures that shared resources are accessed and updated correctly by processes, maintaining a consistent system state.

        key requirements of synchronization mechanisms:

        1.Mutual Exclusion: No two processes can be in the critical section at the same time.

        2.Progress: If no process is in the critical section, a process that wishes to enter the critical section must be able to do so eventually.

        3.Bounded Waiting: There should be a limit on the number of times other processes are allowed to enter the critical section after a process has made a request to enter.

Q8. Process Synchronisation Mechanisms

    Here are some of the mechanisms that fulfill the above process synchronization requirements:
    1. Locks (Mutex):
        A lock (or mutex, short for mutual exclusion) is a flag or variable that prevents multiple processes from accessing a shared resource simultaneously. A process must acquire the lock before entering the critical section and release the lock after exiting it.

        mutex.lock();  // Acquire the lock
        // Critical section
        mutex.unlock();  // Release the lock

    2. Semaphores
        A semaphore is a synchronization primitive used to control access to shared resources. It can be either:
            Binary Semaphore: Acts like a mutex, with values 0 or 1.
            Counting Semaphore: Allows access to a resource for a specified number of processes.

            Semaphore S = 1; // Initialize semaphore to 1 (binary semaphore)

            P(S); // Wait (decrement the semaphore)
            // Critical section
            V(S); // Signal (increment the semaphore)

    3.Readers-Writers Problem:
        The problem is to ensure that multiple readers can read simultaneously, but only one writer can modify the data at a time.

    ### Issues in Process Synchronization:
        1.Deadlock:Occurs when two or more processes wait indefinitely for resources 
        2.starvation:Occurs when a process waits indefinitely
        3.Race-condition:Occurs when multiple processes or threads access shared data concurrently,

Q9. Deadlock
        Deadlock is a situation in concurrent systems where two or more processes are unable to proceed because each one is waiting for others to release a resource.

        Example of Deadlock:
        Consider two processes, P1 and P2, and two resources, R1 and R2:

        Process P1 holds Resource R1 and needs Resource R2 to proceed.
        Process P2 holds Resource R2 and needs Resource R1 to proceed.
        Since neither process can release the resources it holds until it gets the other resource, both processes wait indefinitely, resulting in a deadlock.

        #Conditions for Deadlock:
        1.Mutual Exclusion:Only one process can use a resource at any given time.
        2.Hold and Wait:A process waits for some resources while holding another resource at the same time.
        3.No Preemption:Resources cannot be forcibly removed from a process holding them. The process must voluntarily release the resource.
        4.Circular Wait:A set of processes exists where each process is waiting for a resource that the next process in the chain holds. This forms a circular chain of dependencies.

Q10. Deadlock Handling Techniques

    Deadlock handling is crucial for ensuring the smooth execution of concurrent processes in an operating system. There are four main strategies for dealing with deadlocks:

    1. Deadlock Prevention: Ensure that at least one of the necessary conditions for deadlock (mutual exclusion, hold and wait, no preemption, circular wait) never holds.

    2. Deadlock Avoidance: Dynamically allocate resources while ensuring the system never enters an unsafe state where deadlock could occur.
        #Banker's Algorithm (for avoidance): In this algorithm, every process must declare in advance the maximum number of resources it might need. The system ensures that resources are allocated in such a way that, even in the worst-case scenario, no deadlock will occur.

        eg.Imagine a bank granting loans to customers. The bank won’t approve a loan if there’s a chance that it won’t have enough cash to meet the demands of all other customers. The system checks the safety of each new loan request (resource allocation).

    3. Deadlock Detection: The operating system continuously monitors the system to detect the presence of deadlock using resource allocation

    4.Deadlock Recovery:Terminate one or more of the processes involved in the deadlock to break the cycle. 

    5. Deadlock Ignorance: In this strategy, deadlocks are not detected or avoided. Instead, the system simply assumes that deadlocks will be extremely rare and that rebooting or restarting the system will resolve any deadlock if it occurs.

Q11. Memory Management

    Memory management is a critical function of the operating system (OS), responsible for managing the computer's memory resources efficiently. It involves the allocation, deallocation of memory spaces of process
        *The OS allocates memory to processes when they need it and deallocates memory when they no longer need it.
    
    Fixed partitioning and dynamic partitioning are two approaches used in memory management systems to allocate and manage memory resources.
    1. Fixed Partitioning: In fixed partitioning, the memory is divided into fixed-size blocks (or partitions) when the operating system starts, and these partitions remain constant in size throughout. Each partition can hold exactly one process at a time.
        eg. Consider a system where memory is divided into three partitions of 100 MB, 200 MB, and 300 MB. If a process requests 150 MB, the second partition (200 MB) is allocated, leaving some memory unused, potentially leading to fragmentation.
    
    2. Dynamic Partitioning: In dynamic partitioning, memory is divided into partitions dynamically as processes are loaded into memory. Unlike fixed partitioning, the size of the partition is determined by the size of the process requesting memory.
        eg.Suppose there is 1 GB of RAM. When a process requests 100 MB, the system allocates exactly 100 MB, and the rest of the memory remains free. When another process requests 200 MB, another 200 MB is allocated from the available memory.

        . For instance, after some processes terminate, the system might have 50 MB, 150 MB, and 100 MB blocks available in different parts of the memory, leading to external fragmentation.


Q12. Partition and Memory allocation

    1. First Fit Memory Allocation: The First Fit algorithm allocates the first block of memory that is large enough to satisfy the process's requirements. It scans the memory from the beginning, and as soon as it finds a large enough free partition, it assigns it to the process.

    2. Best Fit Memory Allocation:The Best Fit algorithm scans all available memory blocks and allocates the smallest block that is large enough to satisfy the process’s memory request. This method is designed to minimize the amount of wasted space (i.e., internal fragmentation).

    3. Worst Fit Memory Allocation:The Worst Fit memory allocation algorithm assigns the largest available block of memory to the process. The idea behind this approach is that by leaving larger spaces for future allocations, it reduces the likelihood of having too many small, unusable fragments in memory.

Q13. Paging
    In Operating Systems, Paging is a storage mechanism used to retrieve processes from the secondary storage into the main memory in the form of pages.

    ### Key Concepts in Paging:
        1.Pages: The process's logical memory is divided into fixed-size blocks known as pages.

        2.Frames: The physical memory (RAM) is divided into fixed-size blocks known as frames, which are of the same size as pages. This allows each page of a process to be mapped to a frame in physical memory.

        3.Page Table: A page table is used to keep track of which page is stored in which frame. Each process has its own page table, which maps logical addresses (pages) to physical addresses (frames).

        4.Page Number and Offset:

        The logical address generated by the CPU consists of two parts:
        Page Number: Used to index into the page table.
        Offset: Specifies the exact location within a page.
            The physical address is then generated by combining the frame number with the offset.

Q14. Virtual Memory
        Virtual memory is a concept that lets a computer use more memory than it actually has. It creates an imaginary memory space by combining physical memory (RAM) and secondary storage (like a hard disk). When a program needs more memory than is available in RAM, it temporarily moves some data to the secondary storage. This allows the computer to run larger programs and handle multiple tasks at once

        Imagine you are running multiple applications on your computer: a web browser, a game, and a large photo-editing software.

        *Your computer has 4 GB of physical RAM, but all these applications together need 8 GB of memory to run.

        *The operating system uses virtual memory to manage this situation by creating a virtual address space for each application that appears larger than the actual physical RAM.

        *Let’s say the photo-editing software needs 3 GB of memory, but only 1 GB is loaded into physical RAM at a time. The remaining 2 GB is kept on the hard disk (in a swap file).

        *When the photo-editing software tries to access data that’s not currently in RAM, a page fault occurs, and the OS will swap out some parts of the web browser (or another program) and load the required parts of the photo-editing software into RAM

        ##Another Example
        Think of virtual memory as a desk (RAM) and filing cabinets (disk).

        Imagine you're working on a project and can only fit 3 documents on your desk at once (representing physical RAM).

        However, you have 10 documents that are part of your project, which are stored in the filing cabinet (representing the hard disk).
        Now, when you need to refer to one of the documents that isn’t currently on your desk:

        You’ll put one of the documents from your desk back into the filing cabinet (this is like swapping memory to disk).
        Then, you grab the document you need from the filing cabinet and place it on your desk (this is like a page fault bringing data from disk to RAM).

Q15. Page Replacement Algorithms:
    When memory is full, and the system needs to load new pages into RAM, a page replacement algorithm decides which page to remove. Some common algorithms include:
    1.Least Recently Used (LRU): Replaces the page that hasn’t been used for the longest time.
    2.First In First Out (FIFO): Replaces the oldest page in memory.
    3.Optimal Algorithm: Replaces the page that won’t be needed for the longest period of time in the future (hypothetical, as future requests aren’t known).

Q16. Thrashing
        Thrashing occurs when a computer's virtual memory system is overloaded with too many page faults and spends more time swapping data between RAM and disk than executing actual processes. This causes severe performance degradation as the system is constantly "thrashing" between memory and disk, leaving little time for real work to be done.

Q17. Segmentation
    Segmentation is divided into different variable-sized segments, each representing a logical unit such as a function, data structure, or array.

        In segmentation, the logical address is divided into two parts:
            1. Segment Number (S): Refers to the specific segment.
            2. Offset (O): Refers to the exact location within that segment.

Q18. Disk Management
    Disk Management refers to the techniques  used by an operating system (OS) to manage the storage of data on physical storage devices, such as hard drives, SSDs.

    In disk management, seek time, rotational latency, and transfer time are critical factors affecting the overall performance of disk access.
    1.Seek Time refers to the time it takes for the read/write head of a disk drive to move to the track (or cylinder) where the desired data is located.
    2.Rotational Latency is the time it takes for the desired sector of the disk to rotate under the read/write head after the head has reached the correct track.

QDisk Scheduling Algorithms:

    When multiple processes request I/O from the disk, the OS uses disk scheduling algorithms to determine the most efficient order of operations. The goal is to minimize seek time, which is the time it takes for the disk’s read/write head to move to the correct track.

    1. First Come First Serve (FCFS): Requests are handled in the order they arrive.
    2. Shortest Seek Time First (SSTF): The request closest to the current head position is served next.
    3. SCAN: The disk head moves back and forth across the disk to handle requests (also called the elevator algorithm).
    4. C-SCAN (Circular SCAN): Similar to SCAN but only processes requests in one direction before resetting.

    eg.Queue: 45, 130, 10, 140, 50, 95, 180
        And the disk head is currently at position 100.

        1.FCFS: You explain how FCFS would serve each request in the order it arrives, resulting in significant movement (100 to 45, then 45 to 130, etc.), leading to higher seek time.

        2. SSTF: Here, SSTF would select the closest request to 100, which is 95. Then from 95, it would go to 50, and so on, minimizing the head movement at each step.

        3. SCAN: In SCAN, the disk head would move toward 180 (the largest request), serving 130 and 140 along the way, and then reverse to serve smaller requests like 50, 45, and 10.

        4. C-SCAN: In C-SCAN, the head would serve all requests in the outward direction, reaching 180, then jump back to the beginning to serve requests at lower tracks.

                        <----   Questions ---->
# Segmentation over Paging
    example:
    * A function is 7 KB, and if the system uses paging with 2 KB pages, the function will be split into 4 parts (2 KB for each of the first three pages, and 1 KB for the last page).

    * In paging, these parts can be placed anywhere in memory, possibly scattered, so they're not stored contiguously. While paging eliminates fragmentation, it doesn't respect the logical boundaries of the function. Each part of the function could be stored far apart in physical memory, and it doesn't reflect the function as a single logical unit.

    * By using segmentation, the entire 7 KB function can be treated as a single logical segment. This ensures that the system recognizes it as one entity, and you can apply appropriate access permissions or management policies (such as growing or shrinking) to the function as a whole. Segmentation can help keep the function logically intact, even though physical memory can still be managed using paging.

    * This example shows that segmentation provides a logical structure to the program, whereas paging is more focused on efficient physical memory allocation

